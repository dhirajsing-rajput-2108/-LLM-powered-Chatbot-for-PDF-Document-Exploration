# -LLM-powered-Chatbot-for-PDF-Document-Exploration
Developed a Streamlit web application that utilizes OpenAI's GPT-3.5-turbo LLM to answer user questions about uploaded PDF content. The project involved text extraction, text chunking, embedding generation, and question-answering using LangChain libraries.

# LLM Chat App for PDFs

This Streamlit web application allows users to ask questions about the content of a uploaded PDF document using a large language model (LLM).

## Libraries Used

* Streamlit:  for creating the web app interface (https://streamlit.io/)
* dotenv: for managing environment variables (likely for OpenAI API keys)
* pickle: for saving and loading document embeddings
* PyPDF2: for reading and extracting text from PDFs
* Langchain: a library for building natural language processing pipelines (https://readthedocs.org/projects/langchain/)

## Project Structure

The project consists of a single Python file (`app.py`) containing the entire application logic. 

## Code Execution Flow

1. **App Startup:**
    * The application loads required libraries.
    * The Streamlit sidebar displays the app title, information about its functionality and the libraries used, and the developer's name.

2. **PDF Upload:**
    * The user uploads a PDF document through the Streamlit file uploader.

3. **PDF Processing:**
    * If a PDF is uploaded:
        * The application extracts text from each page of the PDF using PyPDF2.
        * The extracted text is split into smaller chunks using LangChain's RecursiveCharacterTextSplitter.
        * OpenAI embeddings are created for each text chunk using LangChain's OpenAIEmbeddings class. These embeddings are numerical representations of the text used for similarity searches.
        * The embeddings are saved along with the document name using pickle for later use (avoiding re-generating embeddings for the same document).

4. **User Interaction:**
    * The user enters a question about the uploaded PDF document in the text input field.

5. **Question Answering:**
    * The application uses LangChain's FAISS vector store to search for relevant text chunks based on the user query and the saved embeddings.
    * A question is formulated based on the retrieved text chunks and the user query.
    * The question is sent to the OpenAI GPT-3.5-turbo large language model.
    * The response generated by the LLM is displayed in the Streamlit app.

## How to Run the App

1. Make sure you have Python and the required libraries installed (`streamlit`, `dotenv`, `pickle`, `PyPDF2`, `langchain`). You can install them using `pip`:

   **pip install streamlit dotenv pickle PyPDF2 langchain**
   
Clone or download this repository.

Create a file named .env in the project's root directory (refer to LangChain documentation for setting up your OpenAI API key).

Run the app using:
**streamlit run app.py**

This will launch the Streamlit app in your web browser, allowing you to upload PDFs and interact with the LLM chat interface.


